{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/kaggle/working/sample/examples\")\n",
    "import tensorflow as tf\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "import glob\n",
    "from tensorflow.python.ops import math_ops\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_US_annotations = \"C:/Users/KIIT/OneDrive/Desktop/Vedant_Official/vedant projects and works/Datasets/Organ_segmentation_dataset/abdominal_US/abdominal_US/RUS/annotations\"\n",
    "real_US_Images = \"C:/Users/KIIT/OneDrive/Desktop/Vedant_Official/vedant projects and works/Datasets/Organ_segmentation_dataset/abdominal_US/abdominal_US/RUS/images\"\n",
    "artificial_US_annotations = \"C:/Users/KIIT/OneDrive/Desktop/Vedant_Official/vedant projects and works/Datasets/Organ_segmentation_dataset/abdominal_US/abdominal_US/AUS/annotations\"\n",
    "artificial_US_Images = \"C:/Users/KIIT/OneDrive/Desktop/Vedant_Official/vedant projects and works/Datasets/Organ_segmentation_dataset/abdominal_US/abdominal_US/AUS/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_data(image_path,size=None, rgb=True):\n",
    "    img = Image.open(image_path.decode())\n",
    "    if not (size is None):\n",
    "        img = img.resize(size.tolist())\n",
    "    if not rgb:\n",
    "        img = img.convert(\"L\")\n",
    "    else:\n",
    "        img = img.convert(\"RGB\")   \n",
    "    img_data = np.array(img).astype(np.uint8)\n",
    "    img.close()\n",
    "    return img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_color(rgb_arr, mode='rgb_to_str'):\n",
    "  if mode == 'rgb_to_str':\n",
    "    return np.apply_along_axis(','.join, 1, rgb_arr.astype(str))\n",
    "  elif mode == 'str_to_rgb':\n",
    "    return np.array([list(map(int, val.split(\",\"))) for val in rgb_arr.split(\"_\")[0].split(\",\")]).flatten()\n",
    "  else:\n",
    "    raise ValueError(\"Invalid mode. Supported modes are 'rgb_to_str' and 'str_to_rgb'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_segment_dict():\n",
    "    # violet = liver         yellow = kidney \n",
    "    # blue = pancreas        red = vessels\n",
    "    # light blue= adrenals   green = gallbladder\n",
    "    # white = bones          pink = spleen \n",
    "    # black = None\n",
    "    seg_dict = {\n",
    "        '0,0,0_none': 0,\n",
    "        '100,0,100_liver': 1,\n",
    "        '255,255,255_bone': 2,\n",
    "        '0,255,0_gallbladder': 3,\n",
    "        '255,255,0_kidney ': 4,\n",
    "        '0,0,255_pancreas': 5,\n",
    "        '255,0,0_vessels': 6,\n",
    "        '255,0,255_spleen': 7,\n",
    "        '0,255,255_adrenal': 8\n",
    "    }\n",
    "    return seg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentDistances:\n",
    "    def __init__(self, seg_dict):\n",
    "        self.segment_arrs = np.array([convert_color(val,'str_to_rgb') for val in seg_dict.keys()])\n",
    "        self.lookup = seg_dict\n",
    "    \n",
    "    def get_closest_segment(self, seg_img_arr):\n",
    "        distances = np.linalg.norm(self.segment_arrs - seg_img_arr.astype(np.float16), axis=1)\n",
    "        min_idx = np.argmin(distances)\n",
    "        return min_idx\n",
    "    \n",
    "    def segments_to_sparse(self,seg_img):\n",
    "        closest_segments = np.apply_along_axis(self.get_closest_segment, axis=2, arr=seg_img)\n",
    "        closest_segments = closest_segments.astype(np.uint8)\n",
    "        kernel = np.ones((5, 5), np.uint8) \n",
    "        img_erosion = cv2.erode(closest_segments, kernel, iterations=5) \n",
    "        img_dilation = cv2.dilate(img_erosion, kernel, iterations=3)\n",
    "        return img_dilation\n",
    "    \n",
    "    def segments_to_masks(self,seg_sparse_img):\n",
    "        height,width = seg_sparse_img.shape\n",
    "        masks = np.zeros((height,width,len(self.lookup)-1))\n",
    "        kernel = np.ones((5, 5), np.uint8) \n",
    "\n",
    "        for i in range(1,len(self.lookup)):\n",
    "            masks[:,:,i-1] = (seg_sparse_img==i).astype(np.uint8)\n",
    "            masks[:,:,i-1] = cv2.erode(masks[:,:,i-1], kernel, iterations=5) \n",
    "            masks[:,:,i-1] = cv2.dilate(masks[:,:,i-1], kernel, iterations=3)\n",
    "        return masks\n",
    "    \n",
    "    def _preprocess_images(self,filename):\n",
    "        out = tf.numpy_function(load_image_data,[filename,tf.constant((256,256))], tf.uint8)\n",
    "#         out.set_shape((256,256,3))\n",
    "        return out\n",
    "\n",
    "    def _preprocess_annotations(self,filename):\n",
    "        img = self._preprocess_images(filename)\n",
    "        out = tf.numpy_function(self.segments_to_sparse,[img], tf.uint8)\n",
    "#         out.set_shape((1,256,256))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0,0,0_none': 0,\n",
       " '100,0,100_liver': 1,\n",
       " '255,255,255_bone': 2,\n",
       " '0,255,0_gallbladder': 3,\n",
       " '255,255,0_kidney ': 4,\n",
       " '0,0,255_pancreas': 5,\n",
       " '255,0,0_vessels': 6,\n",
       " '255,0,255_spleen': 7,\n",
       " '0,255,255_adrenal': 8}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg_dicts = generate_segment_dict()\n",
    "seg_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KIIT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "seg_dicts = generate_segment_dict()\n",
    "SD = SegmentDistances(seg_dicts)\n",
    "\n",
    "def make_data_set(path,f):\n",
    "    files = glob.glob(path)\n",
    "    file_data = tf.data.Dataset.from_tensor_slices(files)\n",
    "    processed_data = file_data.map(f)\n",
    "    return processed_data\n",
    "\n",
    "tf_train_data = make_data_set(artificial_US_Images + \"/train/*\",SD._preprocess_images)\n",
    "tf_test_data = make_data_set(artificial_US_Images + \"/test/*\",SD._preprocess_images)\n",
    "\n",
    "tf_test_real_data = make_data_set(real_US_Images + \"/test/*\",SD._preprocess_images)\n",
    "\n",
    "\n",
    "tf_train_annot = make_data_set(artificial_US_annotations + \"/train/*\",SD._preprocess_annotations)\n",
    "tf_test_annot = make_data_set(artificial_US_annotations + \"/test/*\",SD._preprocess_annotations)\n",
    "\n",
    "tf_test_real_annot = make_data_set(real_US_annotations + \"/test/*\",SD._preprocess_annotations)\n",
    "\n",
    "\n",
    "tf_dataset = tf.data.Dataset.zip((tf_train_data, tf_train_annot))\n",
    "tf_test = tf.data.Dataset.zip((tf_test_data, tf_test_annot))\n",
    "tf_test_real = tf.data.Dataset.zip((tf_test_real_data, tf_test_real_annot))\n",
    "# def set_shapes(image, label):\n",
    "#     image.set_shape((256, 256, 3))\n",
    "#     label.set_shape((256, 256,1))\n",
    "#     return image, label\n",
    "                    \n",
    "# tf_dataset = tf_dataset.map(set_shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "633 293 213\n",
      "633 293 61\n"
     ]
    }
   ],
   "source": [
    "print(len(tf_train_data),len(tf_test_data),len(tf_test_real_data))\n",
    "print(len(tf_train_annot),len(tf_test_annot),len(tf_test_real_annot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_unet_model(input_shape, OUTPUT_CLASSES):\n",
    "    def unet_model(output_channels:int):\n",
    "        inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "#         inputs = tf.keras.layers.Rescaling(1/255.0)(inputs)\n",
    "        # Downsampling through the model\n",
    "        skips = down_stack(inputs)\n",
    "        x = skips[-1]\n",
    "        skips = reversed(skips[:-1])\n",
    "\n",
    "        # Upsampling and establishing the skip connections\n",
    "        for up, skip in zip(up_stack, skips):\n",
    "            x = up(x)\n",
    "            concat = tf.keras.layers.Concatenate()\n",
    "            x = concat([x, skip])\n",
    "\n",
    "        # This is the last layer of the UNET model\n",
    "        x = tf.keras.layers.Conv2DTranspose(filters=100,\n",
    "                                            kernel_size=3,\n",
    "                                            strides=2,\n",
    "                                            padding='same')(x)\n",
    "        # Extra layer after the UNET to help with smoothing the output\n",
    "        x = tf.keras.layers.Conv2D(filters=OUTPUT_CLASSES,\n",
    "                                   kernel_size=3,\n",
    "                                   strides=1,\n",
    "                                   padding='same',\n",
    "                                   activation=\"softmax\"\n",
    "                                   )(x)\n",
    "        return tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape, include_top=False)\n",
    "    # Use the activations of these layers\n",
    "    layer_names = [\n",
    "        'block_1_expand_relu',\n",
    "        'block_3_expand_relu',\n",
    "        'block_6_expand_relu',\n",
    "        'block_13_expand_relu',\n",
    "        'block_16_project',\n",
    "    ]\n",
    "    base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "    # Create the feature extraction model\n",
    "    down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
    "    down_stack.trainable = False\n",
    "\n",
    "    up_stack = [\n",
    "        pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "        pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "        pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
    "        pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
    "    ]\n",
    "\n",
    "    return unet_model(output_channels=OUTPUT_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9406464/9406464 [==============================] - 3s 0us/step\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)       [(None, 256, 256, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " model_2 (Functional)        [(None, 128, 128, 96),       1841984   ['input_11[0][0]']            \n",
      "                              (None, 64, 64, 144),                                                \n",
      "                              (None, 32, 32, 192),                                                \n",
      "                              (None, 16, 16, 576),                                                \n",
      "                              (None, 8, 8, 320)]                                                  \n",
      "                                                                                                  \n",
      " sequential (Sequential)     (None, 16, 16, 512)          1476608   ['model_2[0][4]']             \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate  (None, 16, 16, 1088)         0         ['sequential[0][0]',          \n",
      " )                                                                   'model_2[0][3]']             \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)   (None, 32, 32, 256)          2507776   ['concatenate_8[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate  (None, 32, 32, 448)          0         ['sequential_1[0][0]',        \n",
      " )                                                                   'model_2[0][2]']             \n",
      "                                                                                                  \n",
      " sequential_2 (Sequential)   (None, 64, 64, 128)          516608    ['concatenate_9[0][0]']       \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenat  (None, 64, 64, 272)          0         ['sequential_2[0][0]',        \n",
      " e)                                                                  'model_2[0][1]']             \n",
      "                                                                                                  \n",
      " sequential_3 (Sequential)   (None, 128, 128, 64)         156928    ['concatenate_10[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate_11 (Concatenat  (None, 128, 128, 160)        0         ['sequential_3[0][0]',        \n",
      " e)                                                                  'model_2[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_transpose_12 (Conv2  (None, 256, 256, 100)        144100    ['concatenate_11[0][0]']      \n",
      " DTranspose)                                                                                      \n",
      "                                                                                                  \n",
      " conv2d_94 (Conv2D)          (None, 256, 256, 9)          8109      ['conv2d_transpose_12[0][0]'] \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6652113 (25.38 MB)\n",
      "Trainable params: 4808209 (18.34 MB)\n",
      "Non-trainable params: 1843904 (7.03 MB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_unet_model([256, 256, 3], 9) # UNET Model\n",
    "model.compile(optimizer='adam',\n",
    "            loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "            metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def jacard_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
    "\n",
    "\n",
    "def jacard_coef_loss(y_true, y_pred):\n",
    "    return -jacard_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n",
      "550\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "imgs,annot = [],[]\n",
    "i = 0\n",
    "for img,an in tf_dataset.as_numpy_iterator():\n",
    "    if i%50==0:\n",
    "        print(i)\n",
    "    imgs.append(img)\n",
    "    annot.append(an)\n",
    "    i+=1\n",
    "# test_out = [[img,annot] for img,annot in x.take(1).as_numpy_iterator()]\n",
    "imgs = np.array(imgs)\n",
    "annot = np.array(annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KIIT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\KIIT\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.6403 - accuracy: 0.8313 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "40/40 [==============================] - 437s 11s/step - loss: 0.6403 - accuracy: 0.8313\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3519 - accuracy: 0.8836 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "40/40 [==============================] - 497s 12s/step - loss: 0.3519 - accuracy: 0.8836\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.3154 - accuracy: 0.8947 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "40/40 [==============================] - 449s 11s/step - loss: 0.3154 - accuracy: 0.8947\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2948 - accuracy: 0.9035 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "40/40 [==============================] - 465s 12s/step - loss: 0.2948 - accuracy: 0.9035\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2838 - accuracy: 0.9074 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "40/40 [==============================] - 582s 15s/step - loss: 0.2838 - accuracy: 0.9074\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2784 - accuracy: 0.9095 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "40/40 [==============================] - 953s 24s/step - loss: 0.2784 - accuracy: 0.9095\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2717 - accuracy: 0.9123 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "40/40 [==============================] - 573s 14s/step - loss: 0.2717 - accuracy: 0.9123\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2673 - accuracy: 0.9139 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "40/40 [==============================] - 412s 10s/step - loss: 0.2673 - accuracy: 0.9139\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2634 - accuracy: 0.9143 WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "40/40 [==============================] - 418s 10s/step - loss: 0.2634 - accuracy: 0.9143\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - ETA: 0s - loss: 0.2610 - accuracy: 0.9155WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "40/40 [==============================] - 339s 8s/step - loss: 0.2610 - accuracy: 0.9155\n"
     ]
    }
   ],
   "source": [
    "# model.fit(x=imgs,y=annot, epochs=10, batch_size=16)\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping( patience=3, verbose=1, restore_best_weights=True)\n",
    "model = model.fit(x=imgs, y=annot, \n",
    "                    batch_size = 16, \n",
    "                    verbose=1, \n",
    "                    epochs=10, \n",
    "                    # validation_data=(X_test, y_test_cat), \n",
    "                    # class_weight=class_weights,\n",
    "                    shuffle=False,\n",
    "                    callbacks=[early_stopping])\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"best.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('best.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(tf_test.batch(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(tf_test_real.batch(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import ConvexHull\n",
    "seg_img = np.array([img for img in tf_train_annot.take(1).as_numpy_iterator()]).reshape((256,256))\n",
    "classes = np.unique(seg_img.flatten())\n",
    "with \n",
    "for cls in classes:\n",
    "    print(classes)\n",
    "    if cls == 0:\n",
    "        continue\n",
    "    mask = (seg_img==classes[1]).astype(np.uint8)\n",
    "    value = np.where(mask)\n",
    "    mask_points = np.array([[y,x] for y,x in zip(value[0],value[1])])\n",
    "    ch = ConvexHull(mask_points)\n",
    "    line_data = f'{cls} '\n",
    "    for v in ch.vertices:\n",
    "        point = mask_points[v] / 256\n",
    "        line_data += \" \".join(map(str,point))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_model = YOLO(\"yolov8n-seg.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
